{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import seaborn\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_percent_reported = None\n",
    "num_classes = 10\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 1% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  \"\"\"Extract a file (tar). If force=True, override the current files and folders\"\"\"\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. STEP ONE: SINGLE DIGIT MNIST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training set (55000, 784) (55000,)\n",
      "Validation set (5000, 784) (5000,)\n",
      "Test set (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../data/MNIST_data/', one_hot=False)\n",
    "train_dataset = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "test_dataset = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "valid_dataset = mnist.validation.images\n",
    "valid_labels = mnist.validation.labels\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 11.668404\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation error: 11.8%\n",
      "Minibatch loss at step 500: 0.735226\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation error: 86.5%\n",
      "Minibatch loss at step 1000: 0.380711\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation error: 88.0%\n",
      "Minibatch loss at step 1500: 0.717082\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation error: 88.4%\n",
      "Minibatch loss at step 2000: 0.502864\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation error: 89.6%\n",
      "Minibatch loss at step 2500: 0.301252\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation error: 89.9%\n",
      "Minibatch loss at step 3000: 0.049315\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation error: 89.6%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "# Defining Params\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Defining the Graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input init and constant init. According to the theory \n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE * IMAGE_SIZE)) \n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE)) \n",
    "  tf_valid_dataset = tf.constant(valid_dataset) \n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE * IMAGE_SIZE, NUM_LABELS])) # mean0,low std dev, random values\n",
    "  biases = tf.Variable(tf.zeros([NUM_LABELS])) # zeros\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases # logistic regression\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # GD Optimizer\n",
    "  \n",
    "  # Predictions for the training, validation, and test data. (Probabilities)\n",
    "  train_prediction = tf.nn.softmax(logits) \n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases) \n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "  \n",
    "# Defining hyper-parameters\n",
    "NUM_STEPS = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  for step in range(NUM_STEPS):\n",
    "    offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + BATCH_SIZE), :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict) \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print('Validation error: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Layers NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 244.827911\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation error: 24.2%\n",
      "Minibatch loss at step 500: 0.713350\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation error: 92.3%\n",
      "Minibatch loss at step 1000: 0.008918\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation error: 93.6%\n",
      "Minibatch loss at step 1500: 0.266572\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation error: 94.0%\n",
      "Minibatch loss at step 2000: 0.229577\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation error: 94.1%\n",
      "Minibatch loss at step 2500: 0.022741\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation error: 94.4%\n",
      "Minibatch loss at step 3000: 0.000835\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation error: 94.4%\n",
      "Test accuracy: 93.6%\n"
     ]
    }
   ],
   "source": [
    "# Defining Params\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_NODES = 1024\n",
    "\n",
    "# Defining the Graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input init and constant init. According to the theory \n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE * IMAGE_SIZE)) \n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE)) \n",
    "  tf_valid_dataset = tf.constant(valid_dataset) \n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  w1 = tf.Variable(tf.truncated_normal([IMAGE_SIZE * IMAGE_SIZE, HIDDEN_NODES])) \n",
    "  b1 = tf.Variable(tf.zeros([HIDDEN_NODES])) \n",
    "  w2 = tf.Variable(tf.truncated_normal([HIDDEN_NODES, NUM_LABELS])) \n",
    "  b2 = tf.Variable(tf.zeros([NUM_LABELS])) \n",
    "  \n",
    "  def twoNN(X):\n",
    "      return tf.matmul(tf.nn.relu(tf.matmul(X, w1) + b1), w2) + b2\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = twoNN(tf_train_dataset) \n",
    "  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # GD Optimizer\n",
    "  \n",
    "  # Predictions \n",
    "  train_prediction = tf.nn.softmax(logits) \n",
    "  valid_prediction = tf.nn.softmax(twoNN(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(twoNN(tf_test_dataset))\n",
    "  \n",
    "# Defining hyper-parameters\n",
    "NUM_STEPS = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  for step in range(NUM_STEPS):\n",
    "    offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + BATCH_SIZE), :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict) \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print('Validation error: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN LeNet-5-like convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (55000, 28, 28, 1) (55000,)\n",
      "Validation set (5000, 28, 28, 1) (5000,)\n",
      "Test set (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype(np.float32)\n",
    "  labels = labels.astype(np.int32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 10.894684\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 500: 0.403061\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 1000: 0.342183\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 94.5%\n",
      "Minibatch loss at step 1500: 0.201191\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 96.1%\n",
      "Minibatch loss at step 2000: 0.041332\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 96.8%\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "PATCH_SIZE = 5\n",
    "DEPTH_1 = 32\n",
    "DEPTH_2 = 64\n",
    "HIDDEN_NODES = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # 5x5 Filter, depth 16 \n",
    "  conv1_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, NUM_CHANNELS, DEPTH_1], stddev=0.1))\n",
    "  conv1_biases = tf.Variable(tf.zeros([DEPTH_1]))\n",
    "  \n",
    "  # 5x5 Filter, depth 16\n",
    "  conv2_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH_1, DEPTH_2], stddev=0.1))\n",
    "  conv2_biases = tf.Variable(tf.constant(1.0, shape=[DEPTH_2]))\n",
    "\n",
    "  # Fully connected depth 256\n",
    "  fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * DEPTH_2, HIDDEN_NODES],\n",
    "                                                   stddev=0.1))\n",
    "  fc1_biases = tf.Variable(tf.constant(1.0, shape=[HIDDEN_NODES]))\n",
    "  \n",
    "  fc2_weights = tf.Variable(tf.truncated_normal([HIDDEN_NODES, NUM_LABELS], stddev=0.1))\n",
    "  fc2_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    \n",
    "    # 2D Conv Layer\n",
    "    conv = tf.nn.conv2d(data, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # 2D Conv + ReLU + MaxPooling\n",
    "    conv = tf.nn.conv2d(pool, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool, [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "        \n",
    "    # Fully connected  \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "  \n",
    "NUM_STEPS = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(NUM_STEPS):\n",
    "    offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "    batch_data = train_dataset[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
